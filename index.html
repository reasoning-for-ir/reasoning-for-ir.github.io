<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Reasoning for IR & IR for Reasoning Tutorial">
  <meta name="keywords" content="Reasoning for IR, IR for Reasoning, Tutorial, Information Retrieval, Reinforcement Learning, Neuro-symbolic AI, Bayesian Inference">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Reasoning for IR & IR for Reasoning Tutorial</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!-- Home link -->
      <a class="navbar-item" href="https://reasoning-for-ir.github.io/">
        <span class="icon">
          <i class="fas fa-home"></i>
        </span>
      </a>
    </div>
  </div>
</nav>



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <!-- <h1 class="title is-1 publication-title">
            Tutorial:<br>
            <span class="subtitle-line">Reasoning for IR &amp; IR for Reasoning</span>
          </h1> -->
          <h1 class="title is-1 publication-title has-text-centered">
            <div class="title-lines">
              <!-- <span class="tutorial-label">ECIR 2026 Tutorial:</span> -->
              <span class="tutorial-label">Tutorial:</span>

              <span class="main-title">Reasoning for IR &amp; IR for Reasoning</span>
              <!-- <span class="location-label">Delft, The Netherlands</span> -->
            </div>
          </h1>
          <!-- Speakers -->
          <div class="speakers">
            <div class="columns is-centered is-multiline">
          
              <!-- Mohanna -->
              <div class="column is-one-quarter has-text-centered">
                <figure class="image is-128x128 is-inline-block">
                  <img class="is-rounded speaker-photo" src="./static/images/presenters/mohanna.jpg" alt="Mohanna Hoveyda">
                </figure>
                <p class="title is-5 mt-3">
                  <a href="https://mohannahoveyda.com">Mohanna Hoveyda</a><sup>1</sup>
                </p>
              </div>
          
              <!-- Panagiotis -->
              <div class="column is-one-quarter has-text-centered">
                <figure class="image is-128x128 is-inline-block">
                  <img class="is-rounded speaker-photo" src="./static/images/presenters/panagiotis.jpeg" alt="Panagiotis Eustratiadis">
                </figure>
                <p class="title is-5 mt-3">
                  <a href="#">Panagiotis Eustratiadis</a><sup>2</sup>
                </p>
              </div>
          
              <!-- Arjen -->
              <div class="column is-one-quarter has-text-centered">
                <figure class="image is-128x128 is-inline-block">
                  <img class="is-rounded speaker-photo" src="./static/images/presenters/arjen.jpeg" alt="Arjen P. de Vries">
                </figure>
                <p class="title is-5 mt-3">
                  <a href="https://www.cs.ru.nl/~arjen/">Arjen P. de Vries</a><sup>1</sup>
                </p>
              </div>
          
              <!-- Maarten -->
              <div class="column is-one-quarter has-text-centered">
                <figure class="image is-128x128 is-inline-block">
                  <img class="is-rounded speaker-photo" src="./static/images/presenters/maarten.jpeg" alt="Maarten de Rijke">
                </figure>
                <p class="title is-5 mt-3">
                  <a href="https://staff.fnwi.uva.nl/m.derijke/">Maarten de Rijke</a><sup>2</sup>
                </p>
              </div>
          
            </div>
          
            <!-- Affiliations -->
            <div class="is-size-6 has-text-centered mt-3">
              <p>
                <sup>1</sup> Radboud University
                <img class="affiliation-logo" src="./static/images/logos/radboud.svg" alt="Radboud University logo">
                &nbsp;&nbsp;
                <sup>2</sup> University of Amsterdam
                <img class="affiliation-logo" src="./static/images/logos/uva.png" alt="University of Amsterdam logo">
              </p>
            </div>
          </div>

          <!-- Links -->
          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- Slides -->
              <span class="link-block">
                <a href="./static/slides/tutorial_slides.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-powerpoint"></i>
                  </span>
                  <span>Slides (Coming Soon)</span>
                </a>
              </span>

              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Recording (Coming Soon)</span>
                </a>
              </span>

            </div>
          </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">About this tutorial</h2>
        <div class="content has-text-justified">
          <p>
            Information retrieval has long focused on ranking documents by semantic relatedness. 
            Yet many real-world information needs demand more: enforcement of logical constraints, multi-step inference, and synthesis of multiple pieces of evidence. Addressing these requirements is, at its core, a problem of reasoning. 
          </p>
          <p>
            Across AI communities, researchers are developing diverse solutions for the problem 
            of reasoning, from inference-time strategies and post-training of LLMs, to neuro-symbolic systems, 
            Bayesian and probabilistic frameworks, geometric representations, and energy-based models. 
            These efforts target the same problem: to move beyond pattern-matching systems toward structured, verifiable inference. 
            However, they remain scattered across disciplines, making it difficult for IR researchers to identify the most relevant ideas and opportunities.
          </p>
          <p>
            To help navigate the landscape, in this tutorial we organize existing reasoning methods into three axes: 
          </p>
          <ol type="i" style="margin-left: 1.5em;">
            <li><i>Representational adequacy</i></li>
            <li><i>Mechanisms of inference verification and learning</i></li>
            <li><i>Computational viability for IR applications</i></li>
          </ol>
          <p>
            By mapping current methods onto these axes, we expose their trade-offs and complementarities, 
            highlight where IR can benefit from cross-disciplinary advances, and show how IR can, in turn, 
            serve as a testbed for reasoning: forcing methods to operate at scale, remain grounded in evidence, and be evaluated systematically. 
            The tutorial will equip participants with both a conceptual framework and practical guidance 
            for building reasoning-capable IR systems, while situating IR as a domain that both benefits and 
            contributes to the broader development of reasoning methodologies.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section" id="schedule">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Schedule</h2>
    <div class="content has-text-justified">
      <table class="table is-fullwidth">
        <thead>
          <tr>
            <th style="width: 12%;">Time</th>
            <th style="width: 58%;">Session</th>
            <th style="width: 15%;">Speaker(s)</th>
            <th style="width: 15%;">Duration</th>
          </tr>
        </thead>
        <tbody>
          <!-- Part 1 -->
          <tr class="is-selected">
            <td colspan="4"><strong>Part I — Introduction &amp; Motivation</strong></td>
          </tr>
          <tr>
            <td>09:00 – 09:15</td>
            <td>Why reasoning is central for IR; overview of tutorial structure and goals.</td>
            <!-- <td>Motivation and overview of the tutorial. Why reasoning is central to the future of Information Retrieval.</td> -->
            <td>x</td>
            <td>15 min</td>
          </tr>

          <!-- Part 2 -->
          <tr class="is-selected">
            <td colspan="4"><strong>Part II — Reasoning: Definition & its challenges in IR</strong></td>
          </tr>
          <tr>
            <td>09:15 – 09:45</td>
            <td>Definition of reasoning; empirical and theoretical challenges in IR.</td>
            <td>x</td>
            <td>30 min</td>
          </tr>

          <!-- Part 3 -->
          <tr class="is-selected">
            <td colspan="4"><strong>Part III — Methodological Families</strong></td>
          </tr>
          <tr>
            <td>09:45 – 10:30</td>
            <td><i>LLM-based Approaches:</i> inference-time, self-consistency, and RL-based reasoning.</td>
            <td>x</td>
            <td>45 min</td>
          </tr>
          <tr>
            <td>10:30 – 10:45 </td>
            <td><i>Neuro-Symbolic Reasoning:</i> Logical foundations of IR, solvers, probabilistic logic programming, and hybrid pipelines.</td>
            <td>x</td>
            <td>15 min</td>
          </tr>
          <tr>
            <td>10:45 – 11:00</td>
            <td><i>Probabilistic Frameworks:</i> Bayesian reasoning and uncertainty modeling in IR.</td>
            <td>x</td>
            <td>15 min</td>
          </tr>
          <tr>
            <td>11:00 – 11:15</td>
            <td><i>Alternative Representations and Optimization Models:</i> geometric, set-compositional, and energy-based reasoning frameworks.</td>
            <td>x</td>
            <td>15 min</td>
          </tr>


          <tr class="is-selected">
            <td colspan="4"><strong>Part IV — Bridging Current Methodologies &amp; Future Directions</strong></td>
          </tr>
          <tr>
            <td>11:15 – 11:45</td>
            <td>Comparative analysis of existing reasoning paradigms along key dimensions: representational adequacy, inference verifiability, and computational viability.</td>
            <td>x</td>
            <td>30 min</td>
          </tr>
          <tr>
            <td>11:45 – 12:00</td>
            <td>Identification of open research challenges, gaps across methodological lines, and opportunities for future work on reasoning-centric IR systems.</td>
            <td>x</td>
            <td>15 min</td>
          </tr>
        </tbody>
      </table>
    </div>
  </div>
</section>

<!-- <section class="Part" id="reading-list">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Reading List</h2>

    <div class="content has-text-justified">
      <p>
        Below are the key references discussed in this tutorial, organized by topic. 
        Bold papers are discussed in depth.
      </p>

      <h3 class="title is-5 mt-5">Part II</h3>
      <ul>
        <li><strong>[1]</strong> Author et al. <em>Title of Paper 1</em>. <span class="year">Year</span>.</li>
      </ul>

      <h3 class="title is-5 mt-5">Part III</h3>
      <ul>
        <li><strong>[1]</strong> Author et al. <em>Title of Paper 1</em>. <span class="year">Year</span>.</li>
      </ul>

      <h3 class="title is-5 mt-5">Part IV</h3>
      <ul>
        <li><strong>[1]</strong> Author et al. <em>Title of Paper 1</em>. <span class="year">Year</span>.</li>
      </ul>
    </div>
  </div>
</section> -->

<!-- <section class="Part" id="reading-list-generated">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Reading List</h2>
    <div class="content has-text-justified">
      <p> Below are the key references discussed in this tutorial, organized by topic. Bold papers are discussed in depth. </p>
      <h3 class="title is-5 mt-5">Part II — Definition & Challenges in IR</h3>
      <ul>
<li><strong>[1]</strong> Orion Weller, Dawn J. Lawrie, Benjamin Van Durme. <em>NevIR: Negation in Neural Information Retrieval</em>. EACL. 2024.</li>
<li><strong>[2]</strong> Roxana Petcu, Samarth Bhargav, Maarten de Rijke et al.. <em>A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers</em>. EMNLP. 2025.</li>
<li><strong>[3]</strong> Coen van den Elsen, Francien Barkhof, Thijmen Nijdam et al.. <em>Reproducing NevIR: Negation in Neural Information Retrieval</em>. SIGIR 2025. 2025.</li>
<li><strong>[4]</strong> Wenhao Zhang, Mengqi Zhang, Shiguang Wu et al.. <em>ExcluIR: Exclusionary neural information retrieval</em>. AAAI. 2025.</li>
<li><strong>[5]</strong> Chaitanya Malaviya, Peter Shaw, Ming-Wei Chang et al.. <em>QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations</em>. ACL. 2023.</li>
<li><strong>[6]</strong> Hongjin Su, Howard Yen, Mengzhou Xia et al.. <em>BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval</em>. ICLR. 2025.</li>
<li><strong>[7]</strong> Zijian Chen, Xueguang Ma, Shengyao Zhuang et al.. <em>BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent</em>. 2025.</li>
<li><strong>[8]</strong> Julian Killingback, Hamed Zamani. <em>Benchmarking Information Retrieval Models on Complex Retrieval Tasks</em>. 2025.</li>
<li><strong>[9]</strong> Orion Weller, Michael Boratko, Iftekhar Naim et al.. <em>On the Theoretical Limitations of Embedding-Based Retrieval</em>. 2025.</li>
<li><strong>[10]</strong> Adam Tauman Kalai, Ofir Nachum, Santosh S. Vempala et al.. <em>Why Language Models Hallucinate</em>. 2025.</li>
      </ul>
      <h3 class="title is-5 mt-5">Part III — Methodological Families</h3>
      <h4 class="title is-6 mt-3">LLM inference-time strategies and optimization for reasoning</h4>
      <ul>
<li><strong>[1]</strong> Jason Wei, Xuezhi Wang, Dale Schuurmans et al.. <em>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</em>. NeurIPS. 2022.</li>
<li><strong>[2]</strong> Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot et al.. <em>Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions</em>. ACL. 2023.</li>
<li><strong>[3]</strong> Aman Madaan, Niket Tandon, Prakhar Gupta et al.. <em>SELF-REFINE: iterative refinement with self-feedback</em>. NeurIPS. 2023.</li>
<li><strong>[4]</strong> Jonas H\"ubotter, Sascha Bongni, Ido Hakimi et al.. <em>Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs</em>. Adaptive Foundation Models: Evolving AI for Personalized and Efficient Learning. 2024.</li>
<li><strong>[5]</strong> Xuezhi Wang, Denny Zhou. <em>Chain-of-thought reasoning without prompting</em>. NeurIPS. 2024.</li>
<li><strong>[6]</strong> Laura Ruis, Maximilian Mozes, Juhan Bae et al.. <em>Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models</em>. ICLR. 2025.</li>
      </ul>
      <h4 class="title is-6 mt-3">LLMs + RL</h4>
      <ul>
<li><strong>[1]</strong> Long Ouyang, Jeffrey Wu, Xu Jiang et al.. <em>Training language models to follow instructions with human feedback</em>. NeurIPS. 2022.</li>
<li><strong>[2]</strong> DeepSeek AI. <em>DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning</em>. Nature. 2025.</li>
<li><strong>[3]</strong> Bowen Jin, Hansi Zeng, Zhenrui Yue et al.. <em>Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</em>. COLM. 2025.</li>
<li><strong>[4]</strong> Yang Yue, Zhiqi Chen, Rui Lu et al.. <em>Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</em>. AI for Math Workshop @ ICML 2025. 2025.</li>
<li><strong>[5]</strong> Rajkumar Ramamurthy and. <em>Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization</em>. ICLR. 2023.</li>
      </ul>
      <h4 class="title is-6 mt-3">Neuro-symbolic approaches:</h4>
      <ul>
<li><strong>[1]</strong> C J Van Rijsbergen. <em>A new theoretical framework for information retrieval</em>. SIGIR Forum. 1986.</li>
<li><strong>[2]</strong> Theo Olausson, Alex Gu, Ben Lipkin et al.. <em>LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers</em>. EMNLP. 2023.</li>
<li><strong>[3]</strong> Aliakbar Nafar, Kristen Brent Venable, Parisa Kordjamshidi. <em>Reasoning over uncertain text by generative large language models</em>. AAAI. 2025.</li>
<li><strong>[4]</strong> Lionel Wong, Katherine M. Collins, Lance Ying et al.. <em>Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models</em>. 2025.</li>
<li><strong>[5]</strong> Lionel Wong, Gabriel Grand, Alexander K. Lew et al.. <em>From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought</em>. 2023.</li>
      </ul>
      <h4 class="title is-6 mt-3">Probabilistic and Bayesian Frameworks</h4>
      <ul>
<li><strong>[1]</strong> Linlu Qiu, Fei Sha, Kelsey R. Allen et al.. <em>Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models</em>. 2025.</li>
<li><strong>[2]</strong> Matthew Douglas Hoffman, Du Phan, David Dohan et al.. <em>Training Chain-of-Thought via Latent-Variable Inference</em>. NeurIPS. 2023.</li>
<li><strong>[3]</strong> Yu Feng, Ben Zhou, Weidong Lin et al.. <em>BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models</em>. ICLR. 2025.</li>
<li><strong>[4]</strong> Zhangyue Yin, Qiushi Sun, Qipeng Guo et al.. <em>Reasoning in flux: Enhancing large language models reasoning through uncertainty-aware adaptive guidance</em>. ACL. 2024.</li>
      </ul>
      <h4 class="title is-6 mt-3">Alternative representation spaces and optimization approaches:</h4>
      <ul>
<li><strong>[1]</strong> Tejas Chheda, Purujit Goyal, Trang Tran et al.. <em>Box Embeddings: An open-source library for representation learning using geometric structures</em>. EMNLP. 2021.</li>
<li><strong>[2]</strong> Antonios Minas Krasakis, Andrew Yates, Evangelos Kanoulas. <em>Constructing Set-Compositional and Negated Representations for First-Stage Ranking</em>. 2025.</li>
<li><strong>[3]</strong> Tiansi Dong, Mateja Jamnik, Pietro Liò. <em>Neural Reasoning for Sure Through Constructing Explainable Models</em>. AAAI. 2025.</li>
<li><strong>[4]</strong> Weize Chen, Xu Han, Yankai Lin et al.. <em>Hyperbolic Pre-Trained Language Model</em>. IEEE ACM Trans. Audio Speech Lang. Process.. 2024.</li>
<li><strong>[5]</strong> Menglin Yang, Aosong Feng, Bo Xiong et al.. <em>Hyperbolic Fine-tuning for Large Language Models</em>. 2024.</li>
<li><strong>[6]</strong> Menglin Yang, Aosong Feng, Bo Xiong et al.. <em>Enhancing LLM Complex Reasoning Capability through Hyperbolic Geometry</em>. ICML 2024 Workshop on LLMs and Cognition. 2024.</li>
<li><strong>[7]</strong> Yilun Du, Jiayuan Mao, Joshua B. Tenenbaum. <em>Learning Iterative Reasoning through Energy Diffusion</em>. ICML. 2024.</li>
<li><strong>[8]</strong> Alexi Gladstone, Ganesh Nanduru, Md Mofijul Islam et al.. <em>Energy-Based Transformers are Scalable Learners and Thinkers</em>. 2025.</li>
      </ul>
    </div>
  </div>
</section>
 -->
 <section class="Part" id="reading-list-generated">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Reading List</h2>
    <div class="content has-text-justified">
      <p>Below are the references cited in the proposal, organized by tutorial part. Each list uses a title-first style with an author-year suffix, and titles are ready to become hyperlinks later.</p>
      <h3 class="title is-5 mt-5">Part II — Definition & Challenges in IR</h3>
      <ul>
<li><a href="#" data-key="DBLP:conf/eacl/WellerLD24">NevIR: Negation in Neural Information Retrieval</a> (Weller et al., 2024)</li>
<li><a href="#" data-key="petcu2025comprehensive">A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers</a> (Petcu et al., 2025)</li>
<li><a href="#" data-key="van_den_Elsen_2025">Reproducing NevIR: Negation in Neural Information Retrieval</a> (van den Elsen et al., 2025)</li>
<li><a href="#" data-key="zhang2025excluir">ExcluIR: Exclusionary neural information retrieval</a> (Zhang et al., 2025)</li>
<li><a href="#" data-key="malaviya-etal-2023-quest">QUEST: A Retrieval Dataset of Entity-Seeking Queries with Implicit Set Operations</a> (Malaviya et al., 2023)</li>
<li><a href="#" data-key="DBLP:conf/iclr/SuYXSMWLSST0YA025">BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval</a> (Su et al., 2025)</li>
<li><a href="#" data-key="DBLP:journals/corr/abs-2508-06600">BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent</a> (Chen et al., 2025)</li>
<li><a href="#" data-key="killingback2025benchmarkinginformationretrievalmodels">Benchmarking Information Retrieval Models on Complex Retrieval Tasks</a> (Killingback &amp; Zamani, 2025)</li>
<li><a href="#" data-key="weller2025theoreticallimitationsembeddingbasedretrieval">On the Theoretical Limitations of Embedding-Based Retrieval</a> (Weller et al., 2025)</li>
<li><a href="#" data-key="kalai2025language">Why Language Models Hallucinate</a> (Kalai et al., 2025)</li>
      </ul>
      <h3 class="title is-5 mt-5">Part III — Methodological Families</h3>
      <h4 class="title is-6 mt-3">LLM inference-time strategies and optimization for reasoning</h4>
      <ul>
<li><a href="#" data-key="DBLP:conf/nips/Wei0SBIXCLZ22">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</a> (Wei et al., 2022)</li>
<li><a href="#" data-key="DBLP:conf/acl/TrivediBKS23">Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions</a> (Trivedi et al., 2023)</li>
<li><a href="#" data-key="madaan23">SELF-REFINE: iterative refinement with self-feedback</a> (Madaan et al., 2023)</li>
<li><a href="#" data-key="hbotter2024efficiently">Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs</a> (H\"ubotter et al., 2024)</li>
<li><a href="#" data-key="10.5555/3737916.3740039">Chain-of-thought reasoning without prompting</a> (Wang &amp; Zhou, 2024)</li>
<li><a href="#" data-key="DBLP:conf/iclr/RuisMBKGLKRGB25">Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models</a> (Ruis et al., 2025)</li>
      </ul>
      <h4 class="title is-6 mt-3">LLMs + RL</h4>
      <ul>
<li><a href="#" data-key="ouyang2022training">Training language models to follow instructions with human feedback</a> (Ouyang et al., 2022)</li>
<li><a href="#" data-key="DBLP:journals/nature/GuoYZSWZXZMBZY025">DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning</a> (AI, 2025)</li>
<li><a href="#" data-key="jin2025searchr">Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning</a> (Jin et al., 2025)</li>
<li><a href="#" data-key="yue2025does">Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?</a> (Yue et al., 2025)</li>
<li><a href="#" data-key="ramamurthy2022reinforcement">Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization</a> (and, 2023)</li>
      </ul>
      <h4 class="title is-6 mt-3">Neuro-symbolic approaches:</h4>
      <ul>
<li><a href="#" data-key="10.1145/24634.24635">A new theoretical framework for information retrieval</a> (Van Rijsbergen, 1986)</li>
<li><a href="#" data-key="Olausson_2023">LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers</a> (Olausson et al., 2023)</li>
<li><a href="#" data-key="10.1609/aaai.v39i23.34674">Reasoning over uncertain text by generative large language models</a> (Nafar et al., 2025)</li>
<li><a href="#" data-key="wong2025modelingopenworldcognitionondemand">Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models</a> (Wong et al., 2025)</li>
<li><a href="#" data-key="wong2023wordmodelsworldmodels">From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought</a> (Wong et al., 2023)</li>
      </ul>
      <h4 class="title is-6 mt-3">Probabilistic and Bayesian Frameworks</h4>
      <ul>
<li><a href="#" data-key="qiu2025bayesianteachingenablesprobabilistic">Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models</a> (Qiu et al., 2025)</li>
<li><a href="#" data-key="hoffman2023training">Training Chain-of-Thought via Latent-Variable Inference</a> (Hoffman et al., 2023)</li>
<li><a href="#" data-key="feng2025birdtrustworthybayesianinference">BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models</a> (Feng et al., 2025)</li>
<li><a href="#" data-key="yin2024reasoning">Reasoning in flux: Enhancing large language models reasoning through uncertainty-aware adaptive guidance</a> (Yin et al., 2024)</li>
      </ul>
      <h4 class="title is-6 mt-3">Alternative representation spaces and optimization approaches:</h4>
      <ul>
<li><a href="#" data-key="chheda-etal-2021-box">Box Embeddings: An open-source library for representation learning using geometric structures</a> (Chheda et al., 2021)</li>
<li><a href="#" data-key="krasakis2025constructingsetcompositionalnegatedrepresentations">Constructing Set-Compositional and Negated Representations for First-Stage Ranking</a> (Krasakis et al., 2025)</li>
<li><a href="#" data-key="Dong_Jamnik_Liò_2025">Neural Reasoning for Sure Through Constructing Explainable Models</a> (Dong et al., 2025)</li>
<li><a href="#" data-key="DBLP:journals/taslp/ChenHLHXZLS24">Hyperbolic Pre-Trained Language Model</a> (Chen et al., 2024)</li>
<li><a href="#" data-key="yang2024hyperbolicfinetuninglargelanguage">Hyperbolic Fine-tuning for Large Language Models</a> (Yang et al., 2024)</li>
<li><a href="#" data-key="yang2024enhancing">Enhancing LLM Complex Reasoning Capability through Hyperbolic Geometry</a> (Yang et al., 2024)</li>
<li><a href="#" data-key="du24energydiff">Learning Iterative Reasoning through Energy Diffusion</a> (Du et al., 2024)</li>
<li><a href="#" data-key="gladstone2025energybasedtransformersscalablelearners">Energy-Based Transformers are Scalable Learners and Thinkers</a> (Gladstone et al., 2025)</li>
      </ul>
    </div>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <!-- <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a> -->
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
